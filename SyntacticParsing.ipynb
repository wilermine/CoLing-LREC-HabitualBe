{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccd49aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.2\n",
      "3.6.5\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import spacy\n",
    "from spacy import lookups\n",
    "from spacy import tokenizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk import wordpunct_tokenize\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(spacy.__version__)\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa57cd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def untokenize(words):\n",
    "    \"\"\"\n",
    "    Untokenizing a text undoes the tokenizing operation, restoring\n",
    "    punctuation and spaces to the places that people expect them to be.\n",
    "    Ideally, `untokenize(tokenize(text))` should be identical to `text`,\n",
    "    except for line breaks.\n",
    "    \"\"\"\n",
    "    text = words\n",
    "    step1 = text.replace(\"\\\" \", '\"').replace(\" ''\", '\"').replace('. . .',  '...')\n",
    "    step2 = step1.replace(\" ( \", \" (\").replace(\" ) \", \") \")\n",
    "    step3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\n",
    "    step4 = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", step3)\n",
    "    step5 = step4.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\n",
    "         \"can not\", \"cannot\").replace(\" '\", \"'\")\n",
    "    step6 = step5.replace(\"' \", \"'\")\n",
    "    return step6.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a00f3b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word:\n",
    "    def __init__(self, index, form, lemma, upos, head, deprl, deps, children, siblings):\n",
    "        self.index = index\n",
    "        self.form = form\n",
    "        self.lemma = lemma\n",
    "        self.upos = upos\n",
    "        self.head = head\n",
    "        self.deprl = deprl\n",
    "        self.deps = deps\n",
    "        self.children = children\n",
    "        self.siblings = siblings\n",
    "        \n",
    "    def word_info(self):\n",
    "        print('\\t'.join([self.index, self.form, self.lemma, self.upos, self.head, self.deprl]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "147243b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions\n",
    "\n",
    "#NON-HABITUAL RULES\n",
    "\n",
    "to_precedes_be_list = []\n",
    "aux_children_list = []\n",
    "aux_siblings_list = []\n",
    "verbal_auxilary_list = []\n",
    "copular_verb_list = []\n",
    "other_list = []\n",
    "nonhabitual = []\n",
    "habitual = []\n",
    "\n",
    "\n",
    "def rule_info(rule, text):\n",
    "    print(\"RULE: \", rule)\n",
    "    print(\"TEXT: \", text.strip())\n",
    "\n",
    "#Rule #1: be is preceded by \"to\" or \"na\"\n",
    "def to_precedes_be(previous):\n",
    "    if(previous.lemma == \"to\" or previous.lemma == \"na\"):\n",
    "                #rule_info(\"This is not habitual because it is preceded by 'to' or 'na':\", text)\n",
    "                to_precedes_be_list.append(text.strip())\n",
    "                nonhabitual.append(text.strip())\n",
    "                return True\n",
    "                \n",
    "#Rule #2: be has a sister or a child with the upos or deps of aux\n",
    "def aux_children(be):\n",
    "    for child in be.children:\n",
    "        if(child.upos == \"AUX\" and child.deprl == \"aux\"):\n",
    "            #rule_info(\"This is not habitual because it has a child that is aux/aux:\", text)\n",
    "            aux_children_list.append(text.strip())\n",
    "            nonhabitual.append(text.strip())\n",
    "            return True\n",
    "def aux_siblings(be):\n",
    "    for sibling in be.siblings:\n",
    "        if(sibling.upos == \"AUX\" and sibling.deprl == \"aux\" and sibling.lemma != \"do\"):\n",
    "            #rule_info(\"This is not habitual because it has a sister that is aux/aux:\", text)\n",
    "            aux_siblings_list.append(text.strip())\n",
    "            nonhabitual.append(text.strip())\n",
    "            return True\n",
    "            \n",
    "#HABITUAL RULES\n",
    "            \n",
    "#Rule #1: be has a upos of aux, a deprl of aux and its head has a upos of VERB  \n",
    "def verbal_auxilary(be, wordList):\n",
    "    if(be.upos == \"AUX\" and be.deprl == \"aux\"):\n",
    "        for word in wordList:\n",
    "            if(word.index == be.head and word.upos == \"VERB\"):\n",
    "                #rule_info(\"This instance is habitual because be is aux/aux with a verb as its head:\", text)\n",
    "                verbal_auxilary_list.append(text.strip())\n",
    "                habitual.append(text.strip())\n",
    "                return True\n",
    "                \n",
    "#Rule #2: be has a upos of VERB\n",
    "def copular_verb(be):\n",
    "    if(be.upos == \"VERB\"):\n",
    "        #rule_info(\"This instance is habitual because be is a verb:\", text)\n",
    "        copular_verb_list.append(text.strip())\n",
    "        habitual.append(text.strip())\n",
    "        return True\n",
    "\n",
    "#create word list function\n",
    "def create_word_list(entry, index, wordList, text): #DELETE TEXT FROM THE ARGUMENT HERE\n",
    "    for j, line in enumerate(entry, start=1):\n",
    "        if j == index:\n",
    "            #finds the previous line\n",
    "            previousLine = entry[j-2]\n",
    "            previousLine = previousLine.split()\n",
    "            previous = Word(previousLine[0], previousLine[1], previousLine[2], previousLine[3], previousLine[6], previousLine[7], previousLine[8], children=[], siblings=[])\n",
    "            if(wordList):\n",
    "                wordList.pop()\n",
    "            wordList.append(previous)\n",
    "            \n",
    "            #finds be line\n",
    "            beLine = line.split()\n",
    "            be = Word(beLine[0], beLine[1], beLine[2], beLine[3], beLine[6], beLine[7], beLine[8], children=[], siblings=[])\n",
    "            wordList.append(be)\n",
    "            \n",
    "        else:\n",
    "            currentLine = line.split()\n",
    "            word = Word(currentLine[0], currentLine[1], currentLine[2], currentLine[3], currentLine[6], currentLine[7], currentLine[8], children=[], siblings=[])\n",
    "            wordList.append(word)\n",
    "    \n",
    "    #finding sisters and children\n",
    "    for k, word in enumerate(wordList):\n",
    "        if(word.head == be.index):\n",
    "            be.children.append(word)\n",
    "        elif word.head == be.head and word.form.lower() != \"be\": #changed from \"word.index != be.index\"\n",
    "            be.siblings.append(word)\n",
    "        \n",
    "    return [previous, be]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e95c5498",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#gets the line containing \"be\"\n",
    "\n",
    "filepath = \"/Users/wiler/Documents/College/Junior Year - Summer/Oral History/output/instances/allBeInstances.conllu\"\n",
    "leadingSpaces = len(\"# text = \")\n",
    "count = 0\n",
    "entry = []\n",
    "allBeInstances = []\n",
    "end = False\n",
    "cleared = False\n",
    "pattern = \"[.,\\/#!$%\\^&\\*;:{}=\\-_`~()—\\u2026]+[Bb]e$|^[Bb]e[\\u2026.,\\/#!$%\\^&\\*;:{}=\\-_`~()—]+\"\n",
    "\n",
    "with open(filepath, \"r\", encoding=\"utf_8\") as testConllu: #test this with beLinesOnly\n",
    "    testConllu = testConllu.readlines()\n",
    "    \n",
    "    #goes through each line in an entry\n",
    "    for row in testConllu:\n",
    "        if row.startswith(\"# newdoc_id\") or row.startswith(\"# sent_id\"):\n",
    "            continue\n",
    "        elif row.startswith(\"# text\"):\n",
    "            text = row[leadingSpaces:]\n",
    "        elif(row[0].isnumeric()):\n",
    "            entry.append(row)\n",
    "            if(row == testConllu[-1]): #checks if we are currently at the last line\n",
    "                end = True\n",
    "        else:\n",
    "            end = True\n",
    "            \n",
    "            \n",
    "        #we have reached the end of an entry    \n",
    "        if(end is True):\n",
    "            wordList = []\n",
    "            textTokens = []\n",
    "            beIndex = []\n",
    "            \n",
    "            parsedText = nlp(text) #let me try without stripping\n",
    "            for token in parsedText:\n",
    "                textTokens.append(token.text)\n",
    "            \n",
    "            for i, word in enumerate(textTokens): #might have to change word so that it doesn't get confused with the class\n",
    "                if word.lower() == \"be\" or re.search(pattern, word.lower()):\n",
    "                    beIndex.append(i+1)\n",
    " \n",
    "            for index in beIndex:\n",
    "              \n",
    "                #the function returns an array in the format [previous, be]\n",
    "                \n",
    "                temp = create_word_list(entry, index, wordList, text.strip())\n",
    "                previous = temp[0]\n",
    "                be = temp[1]\n",
    "    \n",
    "                \n",
    "                allBeInstances.append(text.strip())\n",
    "                \n",
    "                #non-habitual rules\n",
    "                if(to_precedes_be(previous)):\n",
    "                    continue\n",
    "                elif(aux_children(be)):\n",
    "                    continue\n",
    "                elif(aux_siblings(be)):\n",
    "                #habitual rules\n",
    "                    continue\n",
    "                elif(verbal_auxilary(be, wordList)):\n",
    "                    continue\n",
    "                elif(copular_verb(be)):\n",
    "                    continue\n",
    "                #unclassified instances\n",
    "                else:\n",
    "                    other_list.append(text.strip())\n",
    "                    continue        \n",
    "            \n",
    "            end = False\n",
    "            entry.clear()\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "#remember to clear entry at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03c8ae25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Non-Habitual Instances:  10339\n",
      "Number of Habitual Instances:  265\n",
      "Number of Unclassified Instances:  675\n",
      "All Be Instances:  11279\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Non-Habitual Instances: \", len(nonhabitual))\n",
    "print(\"Number of Habitual Instances: \", len(habitual))\n",
    "print(\"Number of Unclassified Instances: \", len(other_list))\n",
    "print(\"All Be Instances: \", len(allBeInstances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e080145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#writes the sentences in plain text\n",
    "\n",
    "with open(\"./output/nonhabitual/to_precedes_be.txt\", \"w\", encoding=\"utf=8\") as file:\n",
    "    for line in to_precedes_be_list:\n",
    "        file.write(line + \"\\n\")\n",
    "        \n",
    "with open(\"./output/nonhabitual/aux_children.txt\", \"w\", encoding=\"utf_8\") as file:\n",
    "    for line in aux_children_list:\n",
    "        file.write(line + \"\\n\")\n",
    "\n",
    "with open(\"./output/nonhabitual/aux_siblings.txt\", \"w\", encoding=\"utf_8\") as file:\n",
    "    for line in aux_siblings_list:\n",
    "        file.write(line + \"\\n\")\n",
    "        \n",
    "with open(\"./output/habitual/verbal_auxilary.txt\", \"w\", encoding=\"utf_8\") as file:\n",
    "    for line in verbal_auxilary_list:\n",
    "        file.write(line + \"\\n\")\n",
    "\n",
    "with open(\"./output/habitual/copular_verb.txt\", \"w\", encoding=\"utf_8\") as file:\n",
    "    for line in copular_verb_list:\n",
    "        file.write(line + \"\\n\")\n",
    "        \n",
    "with open(\"./output/habitual/all_habitual.txt\", \"w\", encoding=\"utf_8\") as file:\n",
    "    for line in habitual:\n",
    "        file.write(line + \"\\n\")\n",
    "\n",
    "with open(\"./output/nonhabitual/all_nonhabitual.txt\", \"w\", encoding=\"utf_8\") as file:\n",
    "    for line in nonhabitual:\n",
    "        file.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e6979db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#writes the sentences in ConLLu format\n",
    "\n",
    "parsed = [nlp(line) for line in to_precedes_be_list]\n",
    "with open(\"./output/nonhabitual/to_precedes_be.conllu\", \"w\", encoding=\"utf=8\") as output:\n",
    "    for i, parsedSent in enumerate(parsed):\n",
    "        output.write(\"# newdoc_id = to_precedes_be.txt\" + '\\n')\n",
    "        output.write(\"# sent_id = \" + str(i) + '\\n')\n",
    "        output.write(\"# text = \" + str(parsedSent.text) + '\\n')\n",
    "        \n",
    "        for j, token in enumerate(parsedSent):\n",
    "            word_id = str(j+1) \n",
    "            wordform = token.text\n",
    "            lemma = token.lemma_\n",
    "            upos = token.pos_\n",
    "            xpos = '_'\n",
    "            feats = '_'\n",
    "            deps =  '_'\n",
    "            misc = '_'\n",
    "            if token.dep_== \"ROOT\":\n",
    "                head = '0'\n",
    "                deprel = 'root'\n",
    "            else:\n",
    "                head = str(token.head.i + 1)\n",
    "                deprel = token.dep_.lower()\n",
    "            writeStr = '\\t'.join([word_id, wordform, lemma, upos, xpos, feats, head, deprel, deps, misc]) + '\\n'\n",
    "            output.write(writeStr)\n",
    "        \n",
    "        output.write('\\n')\n",
    "        \n",
    "        \n",
    "parsed = [nlp(line) for line in aux_children_list]\n",
    "with open(\"./output/nonhabitual/aux_children.conllu\", \"w\", encoding=\"utf=8\") as output:\n",
    "    for i, parsedSent in enumerate(parsed):\n",
    "        output.write(\"# newdoc_id = aux_children.txt\" + '\\n')\n",
    "        output.write(\"# sent_id = \" + str(i) + '\\n')\n",
    "        output.write(\"# text = \" + str(parsedSent.text) + '\\n')\n",
    "        \n",
    "        for j, token in enumerate(parsedSent):\n",
    "            word_id = str(j+1) \n",
    "            wordform = token.text\n",
    "            lemma = token.lemma_\n",
    "            upos = token.pos_\n",
    "            xpos = '_'\n",
    "            feats = '_'\n",
    "            deps =  '_'\n",
    "            misc = '_'\n",
    "            if token.dep_== \"ROOT\":\n",
    "                head = '0'\n",
    "                deprel = 'root'\n",
    "            else:\n",
    "                head = str(token.head.i + 1)\n",
    "                deprel = token.dep_.lower()\n",
    "            writeStr = '\\t'.join([word_id, wordform, lemma, upos, xpos, feats, head, deprel, deps, misc]) + '\\n'\n",
    "            output.write(writeStr)\n",
    "        \n",
    "        output.write('\\n')\n",
    "        \n",
    "        \n",
    "parsed = [nlp(line) for line in aux_siblings_list]\n",
    "with open(\"./output/nonhabitual/aux_siblings.conllu\", \"w\", encoding=\"utf=8\") as output:\n",
    "    for i, parsedSent in enumerate(parsed):\n",
    "        output.write(\"# newdoc_id = aux_siblings.txt\" + '\\n')\n",
    "        output.write(\"# sent_id = \" + str(i) + '\\n')\n",
    "        output.write(\"# text = \" + str(parsedSent.text) + '\\n')\n",
    "        \n",
    "        for j, token in enumerate(parsedSent):\n",
    "            word_id = str(j+1) \n",
    "            wordform = token.text\n",
    "            lemma = token.lemma_\n",
    "            upos = token.pos_\n",
    "            xpos = '_'\n",
    "            feats = '_'\n",
    "            deps =  '_'\n",
    "            misc = '_'\n",
    "            if token.dep_== \"ROOT\":\n",
    "                head = '0'\n",
    "                deprel = 'root'\n",
    "            else:\n",
    "                head = str(token.head.i + 1)\n",
    "                deprel = token.dep_.lower()\n",
    "            writeStr = '\\t'.join([word_id, wordform, lemma, upos, xpos, feats, head, deprel, deps, misc]) + '\\n'\n",
    "            output.write(writeStr)\n",
    "        \n",
    "        output.write('\\n')\n",
    "        \n",
    "parsed = [nlp(line) for line in verbal_auxilary_list]        \n",
    "with open(\"./output/habitual/verbal_auxilary.conllu\", \"w\", encoding=\"utf=8\") as output:\n",
    "    for i, parsedSent in enumerate(parsed):\n",
    "        output.write(\"# newdoc_id = verbal_auxilary.txt\" + '\\n')\n",
    "        output.write(\"# sent_id = \" + str(i) + '\\n')\n",
    "        output.write(\"# text = \" + str(parsedSent.text) + '\\n')\n",
    "        \n",
    "        for j, token in enumerate(parsedSent):\n",
    "            word_id = str(j+1) \n",
    "            wordform = token.text\n",
    "            lemma = token.lemma_\n",
    "            upos = token.pos_\n",
    "            xpos = '_'\n",
    "            feats = '_'\n",
    "            deps =  '_'\n",
    "            misc = '_'\n",
    "            if token.dep_== \"ROOT\":\n",
    "                head = '0'\n",
    "                deprel = 'root'\n",
    "            else:\n",
    "                head = str(token.head.i + 1)\n",
    "                deprel = token.dep_.lower()\n",
    "            writeStr = '\\t'.join([word_id, wordform, lemma, upos, xpos, feats, head, deprel, deps, misc]) + '\\n'\n",
    "            output.write(writeStr)\n",
    "        \n",
    "        output.write('\\n')\n",
    "\n",
    "parsed = [nlp(line) for line in copular_verb_list]\n",
    "with open(\"./output/habitual/copular_verb.conllu\", \"w\", encoding=\"utf=8\") as output:\n",
    "    for i, parsedSent in enumerate(parsed):\n",
    "        output.write(\"# newdoc_id = copular_verb.txt\" + '\\n')\n",
    "        output.write(\"# sent_id = \" + str(i) + '\\n')\n",
    "        output.write(\"# text = \" + str(parsedSent.text) + '\\n')\n",
    "        \n",
    "        for j, token in enumerate(parsedSent):\n",
    "            word_id = str(j+1) \n",
    "            wordform = token.text\n",
    "            lemma = token.lemma_\n",
    "            upos = token.pos_\n",
    "            xpos = '_'\n",
    "            feats = '_'\n",
    "            deps =  '_'\n",
    "            misc = '_'\n",
    "            if token.dep_== \"ROOT\":\n",
    "                head = '0'\n",
    "                deprel = 'root'\n",
    "            else:\n",
    "                head = str(token.head.i + 1)\n",
    "                deprel = token.dep_.lower()\n",
    "            writeStr = '\\t'.join([word_id, wordform, lemma, upos, xpos, feats, head, deprel, deps, misc]) + '\\n'\n",
    "            output.write(writeStr)\n",
    "        \n",
    "        output.write('\\n')\n",
    "        \n",
    "parsed = [nlp(line) for line in other_list]\n",
    "with open(\"./output/nonhabitual/other_list.conllu\", \"w\", encoding=\"utf=8\") as output:\n",
    "    for i, parsedSent in enumerate(parsed):\n",
    "        output.write(\"# newdoc_id = other_list.txt\" + '\\n')\n",
    "        output.write(\"# sent_id = \" + str(i) + '\\n')\n",
    "        output.write(\"# text = \" + str(parsedSent.text) + '\\n')\n",
    "        \n",
    "        for j, token in enumerate(parsedSent):\n",
    "            word_id = str(j+1) \n",
    "            wordform = token.text\n",
    "            lemma = token.lemma_\n",
    "            upos = token.pos_\n",
    "            xpos = '_'\n",
    "            feats = '_'\n",
    "            deps =  '_'\n",
    "            misc = '_'\n",
    "            if token.dep_== \"ROOT\":\n",
    "                head = '0'\n",
    "                deprel = 'root'\n",
    "            else:\n",
    "                head = str(token.head.i + 1)\n",
    "                deprel = token.dep_.lower()\n",
    "            writeStr = '\\t'.join([word_id, wordform, lemma, upos, xpos, feats, head, deprel, deps, misc]) + '\\n'\n",
    "            output.write(writeStr)\n",
    "        \n",
    "        output.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fa0582",
   "metadata": {},
   "source": [
    "Selects 10% of each category for random quality checking by annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3789c2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370\n",
      "125\n",
      "67\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "#to_precedes_be\n",
    "sample_size = int(len(to_precedes_be_list) * .10)\n",
    "to_precedes_be_list_random = random.sample(to_precedes_be_list, sample_size)\n",
    "\n",
    "parsed = [nlp(line) for line in to_precedes_be_list_random]\n",
    "with open(\"./output/quality_check/to_precedes_be_random.conllu\", \"w\", encoding=\"utf=8\") as output:\n",
    "    for i, parsedSent in enumerate(parsed):\n",
    "        output.write(\"# newdoc_id = to_precedes_be_random.txt\" + '\\n')\n",
    "        output.write(\"# sent_id = \" + str(i) + '\\n')\n",
    "        output.write(\"# text = \" + str(parsedSent.text) + '\\n')\n",
    "        \n",
    "        for j, token in enumerate(parsedSent):\n",
    "            word_id = str(j+1) \n",
    "            wordform = token.text\n",
    "            lemma = token.lemma_\n",
    "            upos = token.pos_\n",
    "            xpos = '_'\n",
    "            feats = '_'\n",
    "            deps =  '_'\n",
    "            misc = '_'\n",
    "            if token.dep_== \"ROOT\":\n",
    "                head = '0'\n",
    "                deprel = 'root'\n",
    "            else:\n",
    "                head = str(token.head.i + 1)\n",
    "                deprel = token.dep_.lower()\n",
    "            writeStr = '\\t'.join([word_id, wordform, lemma, upos, xpos, feats, head, deprel, deps, misc]) + '\\n'\n",
    "            output.write(writeStr)\n",
    "        \n",
    "        output.write('\\n')\n",
    "\n",
    "\n",
    "#aux_children\n",
    "sample_size = int(len(aux_children_list) * .10)\n",
    "aux_children_list_random = random.sample(aux_children_list, sample_size)\n",
    "print(len(aux_children_list_random))\n",
    "\n",
    "parsed = [nlp(line) for line in aux_children_list_random]\n",
    "\n",
    "with open(\"./output/quality_check/aux_children_random.conllu\", \"w\", encoding=\"utf=8\") as output:\n",
    "    for i, parsedSent in enumerate(parsed):\n",
    "        output.write(\"# newdoc_id = aux_children_random.txt\" + '\\n')\n",
    "        output.write(\"# sent_id = \" + str(i) + '\\n')\n",
    "        output.write(\"# text = \" + str(parsedSent.text) + '\\n')\n",
    "        \n",
    "        for j, token in enumerate(parsedSent):\n",
    "            word_id = str(j+1) \n",
    "            wordform = token.text\n",
    "            lemma = token.lemma_\n",
    "            upos = token.pos_\n",
    "            xpos = '_'\n",
    "            feats = '_'\n",
    "            deps =  '_'\n",
    "            misc = '_'\n",
    "            if token.dep_== \"ROOT\":\n",
    "                head = '0'\n",
    "                deprel = 'root'\n",
    "            else:\n",
    "                head = str(token.head.i + 1)\n",
    "                deprel = token.dep_.lower()\n",
    "            writeStr = '\\t'.join([word_id, wordform, lemma, upos, xpos, feats, head, deprel, deps, misc]) + '\\n'\n",
    "            output.write(writeStr)\n",
    "        \n",
    "        output.write('\\n')\n",
    "        \n",
    "#aux_siblings\n",
    "sample_size = int(len(aux_siblings_list) * .10)\n",
    "aux_siblings_list_random = random.sample(aux_siblings_list, sample_size)\n",
    "print(len(aux_siblings_list_random))\n",
    "parsed = [nlp(line) for line in aux_siblings_list_random]\n",
    "\n",
    "with open(\"./output/quality_check/aux_siblings_random.conllu\", \"w\", encoding=\"utf=8\") as output:\n",
    "    for i, parsedSent in enumerate(parsed):\n",
    "        output.write(\"# newdoc_id = aux_siblings_random.txt\" + '\\n')\n",
    "        output.write(\"# sent_id = \" + str(i) + '\\n')\n",
    "        output.write(\"# text = \" + str(parsedSent.text) + '\\n')\n",
    "        \n",
    "        for j, token in enumerate(parsedSent):\n",
    "            word_id = str(j+1) \n",
    "            wordform = token.text\n",
    "            lemma = token.lemma_\n",
    "            upos = token.pos_\n",
    "            xpos = '_'\n",
    "            feats = '_'\n",
    "            deps =  '_'\n",
    "            misc = '_'\n",
    "            if token.dep_== \"ROOT\":\n",
    "                head = '0'\n",
    "                deprel = 'root'\n",
    "            else:\n",
    "                head = str(token.head.i + 1)\n",
    "                deprel = token.dep_.lower()\n",
    "            writeStr = '\\t'.join([word_id, wordform, lemma, upos, xpos, feats, head, deprel, deps, misc]) + '\\n'\n",
    "            output.write(writeStr)\n",
    "        \n",
    "        output.write('\\n')\n",
    "\n",
    "\n",
    "#other_list\n",
    "sample_size = int(len(other_list) * .10)\n",
    "other_list_random = random.sample(other_list, sample_size)\n",
    "print(len(other_list_random))\n",
    "parsed = [nlp(line) for line in other_list_random]\n",
    "\n",
    "with open(\"./output/quality_check/other_list_random.conllu\", \"w\", encoding=\"utf=8\") as output:\n",
    "    for i, parsedSent in enumerate(parsed):\n",
    "        output.write(\"# newdoc_id = other_list_random.txt\" + '\\n')\n",
    "        output.write(\"# sent_id = \" + str(i) + '\\n')\n",
    "        output.write(\"# text = \" + str(parsedSent.text) + '\\n')\n",
    "        \n",
    "        for j, token in enumerate(parsedSent):\n",
    "            word_id = str(j+1) \n",
    "            wordform = token.text\n",
    "            lemma = token.lemma_\n",
    "            upos = token.pos_\n",
    "            xpos = '_'\n",
    "            feats = '_'\n",
    "            deps =  '_'\n",
    "            misc = '_'\n",
    "            if token.dep_== \"ROOT\":\n",
    "                head = '0'\n",
    "                deprel = 'root'\n",
    "            else:\n",
    "                head = str(token.head.i + 1)\n",
    "                deprel = token.dep_.lower()\n",
    "            writeStr = '\\t'.join([word_id, wordform, lemma, upos, xpos, feats, head, deprel, deps, misc]) + '\\n'\n",
    "            output.write(writeStr)\n",
    "        \n",
    "        output.write('\\n')\n",
    "        \n",
    "#copular_verb and verbal_auxilary do not have to be sampled due to their small quantity and importance; all instances will be checked for habituality and concordance to the rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb5d074d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#puts everything into a csv file where the first column holds the text and the second column states whether the sentence followed the rule or not\n",
    "\n",
    "other_list = []\n",
    "\n",
    "\n",
    "#to_precedes_be\n",
    "with open('./output/quality_check/to_precedes_be.csv', 'w', encoding=\"utf-8-sig\") as csvfile:\n",
    "    filewriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    filewriter.writerow(['Text', '1 or 0'])\n",
    "    for sent in to_precedes_be_list_random:\n",
    "        filewriter.writerow([sent])\n",
    "        \n",
    "#aux_children\n",
    "with open('./output/quality_check/aux_children.csv', 'w', encoding=\"utf-8-sig\") as csvfile:\n",
    "    filewriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    filewriter.writerow(['Text', '1 or 0'])\n",
    "    for sent in aux_children_list_random:\n",
    "        filewriter.writerow([sent])\n",
    "        \n",
    "#aux_siblings\n",
    "with open('./output/quality_check/aux_siblings.csv', 'w', encoding=\"utf-8-sig\") as csvfile:\n",
    "    filewriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    filewriter.writerow(['Text', '1 or 0'])\n",
    "    for sent in aux_siblings_list_random:\n",
    "        filewriter.writerow([sent])\n",
    "        \n",
    "#verbal_auxilary\n",
    "with open('./output/quality_check/verbal_auxilary.csv', 'w', encoding=\"utf-8-sig\") as csvfile:\n",
    "    filewriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    filewriter.writerow(['Text', '1 or 0'])\n",
    "    for sent in verbal_auxilary_list:\n",
    "        filewriter.writerow([sent])\n",
    "        \n",
    "#copular_verb\n",
    "with open('./output/quality_check/copular_verb.csv', 'w', encoding=\"utf-8-sig\") as csvfile:\n",
    "    filewriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    filewriter.writerow(['Text', '1 or 0'])\n",
    "    for sent in copular_verb_list:\n",
    "        filewriter.writerow([sent])\n",
    "\n",
    "#other_list\n",
    "with open('./output/quality_check/other.csv', 'w', encoding=\"utf-8-sig\") as csvfile:\n",
    "    filewriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    filewriter.writerow(['Text', '1 or 0'])\n",
    "    for sent in other_list_random:\n",
    "        filewriter.writerow([sent])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
