{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb144f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import csv\n",
    "import shutil\n",
    "import copy\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from spacy import lookups\n",
    "from spacy import tokenizer\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "#from nltk import nlp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import gensim.models\n",
    "import pickle\n",
    "import joblib\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import WordNetCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e918a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c40c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates the rules input data.\n",
    "#Initializing text is the text of the Sentence\n",
    "# be indicates which 'be' is being looked at in the sentence\n",
    "# # num is the index of the line from the original csv\n",
    "# for eg. if there are multiple 'be' in a sentence, the first will have be = 1, second will have be = 2, etc to distinguish them\n",
    "class Sentence:\n",
    "    habituality = 0 #using a ternary system: -1 = nonhabitual 0 = unclassified, 1 = habitual\n",
    "    pos1 = 0; pos2 = 0; pos3 = 0; pos4 = 0; pos5 = 0; pos6 = 0;\n",
    "    synPar1 = 0; synPar2 = 0; synPar3 = 0; synPar4 = 0; \n",
    "    a1 = 0; a2 = 0; a3 = 0; a4 = 0; a5 = 0; r1 = 0;\n",
    "\n",
    "    def __init__(self, text, be, num):\n",
    "        self.text = text\n",
    "        self.be = be\n",
    "        self.num = num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18255ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "\n",
    "with open(\"gold standard sentences no augmentation.csv\", \"r\", encoding=\"utf_8\") as file:\n",
    "    lines = file.readlines()\n",
    "        \n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4cb00e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bb4bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks the numbers of 'be' and duplicates the sentence if there are more than one 'be' to the input data\n",
    "sen = []\n",
    "for line in range(len(lines)):\n",
    "        be = 1\n",
    "        numBe = 0\n",
    "        parsed = nlp(lines[line])\n",
    "        for i, word in enumerate(parsed):\n",
    "            if (word.text.lower() == \"be\"):\n",
    "                numBe += 1\n",
    "        if (numBe >= 1):\n",
    "            sen.append(Sentence(lines[line], be, line))\n",
    "        while (numBe > 1):\n",
    "            be += 1\n",
    "            sen.append(Sentence(lines[line], be, line))\n",
    "            numBe -= 1\n",
    "for Sentence in range(len(sen)):\n",
    "    sen[Sentence].text = (sen[Sentence].text).replace('\"','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174fc5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sen[1].text)\n",
    "print(len(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bd00cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124d7aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "phonetic_variation = [\"wanna\", \"tryna\", \"gonna\", \"gotta\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6430dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below code takes few words before and after 'be' and analyses these words using the rules (pos1-pos6),(synpar1-synpar6),(A1-A5)and (R1) and updates the specific columns with the same name\n",
    "index = -1\n",
    "senNum = 1\n",
    "numBe = 0\n",
    "extraBe = []\n",
    "beIndices = []\n",
    "count = 0\n",
    "\n",
    "for Sentence in sen:\n",
    "    index += 1 \n",
    "    numBe = 0\n",
    "    if (index > 0 and (sen[index].text == sen[index-1].text) and (sen[index].num == sen[index-1].num)):\n",
    "        senNum += 1\n",
    "    else:\n",
    "        senNum = 1\n",
    "    parsed = nlp(Sentence.text)\n",
    "    for i, word in enumerate(parsed):\n",
    "        if (word.text.lower() == \"be\"):\n",
    "            numBe += 1\n",
    "            #if (numBe == senNum):\n",
    "            if (numBe == 1):\n",
    "                print(Sentence.text)\n",
    "                count +=1\n",
    "                beIndices.append(i)\n",
    "                beChildren = list(word.children)\n",
    "                if(word.head == word):\n",
    "                    beSibling = []\n",
    "                else:\n",
    "                    beSibling = list(word.head.children)\n",
    "                beDep = word.dep_\n",
    "                bePOS = word.pos_\n",
    "                beIndex = i\n",
    "                preprepreprevious = None\n",
    "                prepreprevious = None\n",
    "                preprevious = None\n",
    "                previous = None\n",
    "                after = None\n",
    "                postafter = None\n",
    "                postpostafter = None\n",
    "                if(i >= 4):\n",
    "                    preprepreprevious = parsed[i-4]\n",
    "                if(i >= 3):\n",
    "                    prepreprevious = parsed[i-3]\n",
    "                if(i >= 2):\n",
    "                    preprevious = parsed[i-2]\n",
    "                if(i > 0):\n",
    "                    previous = parsed[i-1]\n",
    "                if(i+1 < len(parsed)):\n",
    "                    after = parsed[i+1]\n",
    "                if(i+2 < len(parsed)):\n",
    "                    postafter = parsed[i+2]\n",
    "                if(i+3 < len(parsed)):\n",
    "                    postpostafter = parsed[i+3]\n",
    "                #pos1\n",
    "                if(previous and (previous.tag_ == \"MD\" or previous.tag_ == \"JJ\" or previous.tag_ == \"TO\")):\n",
    "                    Sentence.pos1 = 1\n",
    "\n",
    "                #pos1 part 2\n",
    "                if(previous and previous.text in phonetic_variation):\n",
    "                    Sentence.pos1 = 1\n",
    "\n",
    "                #pos2\n",
    "                if ((after and previous) and (after.tag_ == \"JJ\" and previous.tag_ != \"PRP\" and not previous.tag_.__contains__(\"NN\"))):\n",
    "                    Sentence.pos2 = 1\n",
    "\n",
    "                #pos3\n",
    "                if ((after and previous) and (after.tag_ == \"IN\" and (previous.tag_ == \"VBZ\" or previous.tag_ == \"VBP\"))):\n",
    "                    Sentence.pos3 = 1\n",
    "\n",
    "                #pos4\n",
    "                if (previous and preprevious) and (previous.tag_ == \"NN\" and preprevious.tag_ == \"JJ\"):\n",
    "                    Sentence.pos4 = 1\n",
    "\n",
    "                #pos5\n",
    "                if (previous and after) and (previous.tag_ == \"RB\" and (after.tag_ == \"PRP\" or after.tag_ == \"DT\")):\n",
    "                    Sentence.pos5 = 1\n",
    "\n",
    "                #pos6\n",
    "                if (previous and preprevious) and (previous.tag_ == \"RB\" and preprevious.tag_.__contains__(\"VB\") or preprevious.tag_ == \"MD\"):\n",
    "                    Sentence.pos6 = 1\n",
    "\n",
    "                #synPar1 -> aux_children:\n",
    "                if len(beChildren) != 0:\n",
    "                    for child in beChildren:\n",
    "                        if child.dep_ == \"aux\" and child.pos_ == \"AUX\" and child.text.lower() != \"do\":\n",
    "                            Sentence.synPar1 = 1\n",
    "                #synPar2 -> aux_siblings\n",
    "                if len(beSibling) != 0:\n",
    "                    for sibling in beSibling:\n",
    "                        if sibling.dep_ == \"aux\" and sibling.pos_ == \"AUX\" and sibling.text.lower() != \"do\" and sibling.text != \"be\":\n",
    "                            Sentence.synPar2 = 1\n",
    "                #synPar3 -> verbal_auxilary\n",
    "                if (word.pos_ == \"AUX\" and word.dep_ == \"aux\" and word.head.pos_ == \"VERB\"):\n",
    "                    Sentence.synPar3 = 1\n",
    "                   \n",
    "                #synPar4 -> copular_verb\n",
    "                if(word.pos_ == \"VERB\"):\n",
    "                    Sentence.synPar4 = 1\n",
    "                #A1\n",
    "                if (((previous and preprevious and prepreprevious) and (previous.text == \"n't\" and ((preprevious.text == \"do\") or (preprevious.text == \"Do\"))) and (prepreprevious.pos_ == \"PRON\" or prepreprevious.pos_ == \"NOUN\")) or ((preprevious and prepreprevious and preprepreprevious) and ((previous.pos_ != \"VERB\" and previous.pos_ != \"AUX\")) and preprevious.text == \"n't\" and ((prepreprevious.text == \"do\") or (prepreprevious.text == \"Do\")) and (preprepreprevious.pos_ == \"PRON\" or preprepreprevious.pos_ == \"NOUN\"))):\n",
    "                    Sentence.a1 = 1\n",
    "\n",
    "                #A2\n",
    "                if (((after) and (after.pos_ == \"PUNCT\" or after.pos_ == \"CCONJ\" or after.pos_ == \"DET\" or after.pos_ == \"INTJ\" or after.pos_ == \"PROPN\"))):\n",
    "                    Sentence.a2 = 1\n",
    "\n",
    "\n",
    "                #A3\n",
    "\n",
    "                if ((previous and previous.pos_ == \"PRON\") or (preprevious and preprevious.pos_ == \"PRON\" and previous.pos_ != \"AUX\" and previous.pos_ != \"VERB\" and previous.pos_ != \"PART\") or (prepreprevious and prepreprevious.pos_ == \"PRON\" and previous.pos_ == \"ADV\" and preprevious.pos_ != \"AUX\" and preprevious.pos_ != \"VERB\")):\n",
    "                    Sentence.a3 = 1\n",
    "\n",
    "\n",
    "                #A4\n",
    "\n",
    "                if (after and after.tag_ == \"VBG\" and previous and previous.pos_ != \"AUX\" and not (previous.pos_ == \"PART\" and Sentence.pos1 == 1)):\n",
    "                    Sentence.a4 = 1\n",
    "\n",
    "            # if (after and after.tag_ == \"VBG\" and previous and previous.pos_ != \"AUX\" and not (previous.pos_ == \"PART\" and previous.tag_  == \"TO\" and not previous.text in phonetic_variation)):\n",
    "                #'Be' is followed by a verb ending in 'ing' and is not preceded by an auxiliary verb, ‘to’, or any of the words in phonetic variation: ‘gonna’, ‘gotta’, ‘wanna’, or ‘tryna’; tends to be Habitual.    \n",
    "                    \n",
    "                #A5\n",
    "\n",
    "                if ((previous and preprevious) and (previous.text == \"n't\" and preprevious.text != \"do\" and preprevious.text != \"Do\")):\n",
    "                    Sentence.a5 = 1\n",
    "                #R1\n",
    "\n",
    "                if ((Sentence.pos1 !=1) and (Sentence.pos2 !=1) and (Sentence.pos3 !=1) and (Sentence.pos4 !=1) and (Sentence.pos5 !=1) and (Sentence.pos6 !=1)):\n",
    "                    Sentence.r1 = 1\n",
    "            else:\n",
    "                numBe -= 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fe538c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Introduces the interaction between the rules based on text analysis\n",
    "for Sentence in sen:\n",
    "    if (Sentence.r1 == 0):\n",
    "        Sentence.synPar3 = 0\n",
    "    if (Sentence.a4 == 1):\n",
    "        Sentence.synPar3 = 0\n",
    "    if (Sentence.a2 == 1 and Sentence.a3 == 1):\n",
    "        Sentence.pos5 = 0\n",
    "    if (Sentence.a4 == 1):\n",
    "        Sentence.a5 == 0\n",
    "    if (Sentence.a3 == 1):\n",
    "        Sentence.synPar2 = 0\n",
    "    if (Sentence.pos6 == 1 or Sentence.synPar1 == 1):\n",
    "        Sentence.a3 = 0\n",
    "    if (Sentence.synPar1 == 1):\n",
    "        Sentence.r1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558ca2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates spreadsheet document\n",
    "with open(\"harrison synpar A-inter test rules coraal_analysis_spreadsheet.csv\", \"w\", encoding=\"utf-8-sig\", newline=\"\") as csvfile:\n",
    "    filewriter = csv.writer(csvfile, delimiter=',',\n",
    "quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    filewriter.writerow([\"Habituality\", \"Sentence\", \"POS1\", \"POS2\", \"POS3\", \"POS4\", \"POS5\", \"POS6\",\"A1\", \"A2\",\"A3\", \"A4\", \"A5\", \"SynPar1\", \"SynPar2\", \"SynPar3\", \"SynPar4\", \"R1\"])\n",
    "    for Sentence in sen:\n",
    "        filewriter.writerow([Sentence.habituality, Sentence.text, Sentence.pos1, Sentence.pos2, Sentence.pos3, Sentence.pos4, Sentence.pos5,Sentence.pos6, Sentence.a1, Sentence.a2, Sentence.a3, Sentence.a4, Sentence.a5, Sentence.synPar1, Sentence.synPar2, Sentence.synPar3, Sentence.synPar4, Sentence.r1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dc6179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preData = pd.read_csv(\"harrison synpar test rules coraal_analysis_spreadsheet.csv\")\n",
    "#import chardet\n",
    "\n",
    "# Detect the encoding of the file\n",
    "#with open(\"harrison synpar A-inter test rules coraal_analysis_spreadsheet.csv\", \"rb\") as f:\n",
    "#    result = chardet.detect(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579712b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the input data which has all the information on rules\n",
    "preData = pd.read_csv(\"harrison synpar A-inter test rules coraal_analysis_spreadsheet.csv\")\n",
    "                      #, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9b2241",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b16be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "    habituality = 0 #using a ternary system: -1 = nonhabitual 0 = unclassified, 1 = habitual\n",
    "    pos1 = 0; pos2 = 0; pos3 = 0; pos4 = 0; pos5 = 0; pos6 = 0; s1 = 0; a2 = 0; a3 = 0; a4 = 0; a5 = 0; a6 = 0;\n",
    "    synPar1 = 0; synPar2 = 0; synPar3 = 0; synPar4 = 0; r1 = 0;\n",
    "    \n",
    "    def __init__(self, text):\n",
    "        self.text = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b0d52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "with open(\"gold standard sentences no augmentation.csv\", \"r\", encoding=\"utf_8\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "i = 0\n",
    "del lines[0]\n",
    "while (i < len(lines)):\n",
    "    if (lines[i] == \"\\\"\\n\" or lines[i] == \"\\\"\" or lines[i] == ' \"\\n'):\n",
    "        del lines[i]\n",
    "    i +=1\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ad335d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a74786",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "for line in lines:\n",
    "    if(line != '\",,,,,\\n'):\n",
    "        sentences.append(Sentence(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb12e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(preData))\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bb243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for handling Multiple 'be'\n",
    "sen = []\n",
    "beIndicies = []\n",
    "numBe = 0\n",
    "extraBe = []\n",
    "\n",
    "data = copy.deepcopy(preData)\n",
    "i = len(preData)-1\n",
    "while (i >= 0):\n",
    "    data.drop(i, inplace=True)\n",
    "    i = i -1\n",
    "    \n",
    "print(len(data))\n",
    "    \n",
    "for Sentence in range(len(sentences)):\n",
    "    numBe = 0\n",
    "    parsed = nlp(sentences[Sentence].text)\n",
    "    for i, word in enumerate(parsed):\n",
    "        if (word.text.lower() == \"be\"):\n",
    "            numBe += 1\n",
    "    if (numBe >= 1):\n",
    "#    if (numBe == 1):\n",
    "        sen.append(sentences[Sentence])\n",
    "    while (numBe > 1):\n",
    "        sen.append(sentences[Sentence])\n",
    "        numBe -= 1\n",
    "\n",
    "print(len(sen))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9b957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for Sentence in range(len(sen)):\n",
    "    numBe = 0\n",
    "    parsed = nlp(sen[Sentence].text)\n",
    "    for i, word in enumerate(parsed):\n",
    "        if (word.text.lower() == \"be\"):\n",
    "            numBe += 1\n",
    "    if (numBe >= 1): \n",
    "#    if (numBe == 1):\n",
    "        data = data.append(preData.iloc[Sentence], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67af087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e431ecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = []\n",
    "pppp = []\n",
    "ppp = []\n",
    "pp = []\n",
    "p = []\n",
    "a = []\n",
    "pa = []\n",
    "ppa = []\n",
    "pppa = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e96daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#appending part of speech of the words before and after 'be'\n",
    "numBe = 1\n",
    "\n",
    "for Sentence in sen:\n",
    "    if (numBe == 1):\n",
    "        numBe = 0\n",
    "        parsed = nlp(Sentence.text)\n",
    "        for i, word in enumerate(parsed):\n",
    "            if (word.text.lower() == \"be\"):\n",
    "                numBe += 1\n",
    "                w.append(word.pos_)\n",
    "                if(i >= 4):\n",
    "                    preprepreprevious = parsed[i-4]\n",
    "                    pppp.append(preprepreprevious.pos_)\n",
    "                else:\n",
    "                    pppp.append(-1)\n",
    "                if(i >= 3):\n",
    "                    prepreprevious = parsed[i-3]\n",
    "                    ppp.append(prepreprevious.pos_)\n",
    "                else:\n",
    "                    ppp.append(-1)\n",
    "                if(i >= 2):\n",
    "                    preprevious = parsed[i-2]\n",
    "                    pp.append(preprevious.pos_)\n",
    "                else:\n",
    "                    pp.append(-1)\n",
    "                if(i > 0):\n",
    "                    previous = parsed[i-1]\n",
    "                    p.append(previous.pos_)\n",
    "                else:\n",
    "                    p.append(-1)\n",
    "                if(i+1 < len(parsed)):\n",
    "                    after = parsed[i+1]\n",
    "                    a.append(after.pos_)\n",
    "                else:\n",
    "                    a.append(-1)\n",
    "                if(i+2 < len(parsed)):\n",
    "                    postafter = parsed[i+2]\n",
    "                    pa.append(postafter.pos_)\n",
    "                else:\n",
    "                    pa.append(-1)\n",
    "                if(i+3 < len(parsed)):\n",
    "                    postpostafter = parsed[i+3]\n",
    "                    ppa.append(postafter.pos_)\n",
    "                else:\n",
    "                    ppa.append(-1)\n",
    "                if(i+4 < len(parsed)):\n",
    "                    postpostpostafter = parsed[i+4]\n",
    "                    pppa.append(postpostpostafter.pos_)\n",
    "                else:\n",
    "                    pppa.append(-1)\n",
    "    else:\n",
    "        numBe -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9693bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.rename(columns = {\"ï»¿Habituality\": \"Habituality\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee2a9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reading the file\n",
    "#Creates unigrams-bigrams for 4 words before and after 'be'\n",
    "#df = pd.read_csv('spam.csv',encoding = 'ISO-8859-1',usecols=['v1','v2'])\n",
    "corpus = [] #empty list\n",
    "wlemmatizer = WordNetLemmatizer()\n",
    "length = len(data['Sentence']) #finding total number of rows\n",
    "be_sen = []\n",
    "for i in data['Sentence']:\n",
    "    start = 0\n",
    "    end = 0\n",
    "    if nlp(i) != parsed:\n",
    "        parsed = nlp(i)\n",
    "        for j, word in enumerate(parsed):\n",
    "            if word.text.lower() == \"be\":\n",
    "                if(j >= 4):\n",
    "                    start = j-4\n",
    "                else:\n",
    "                    start = 0\n",
    "                if(len(i) > j+4):\n",
    "                    end = j+4    \n",
    "                else:\n",
    "                    end = len(i)\n",
    "        be_sen.append(parsed[start:end])\n",
    "\n",
    "print(len(be_sen))\n",
    "for i in range(length):\n",
    "\trev = re.sub('[^a-zA-Z]',' ',str(be_sen[i]))\n",
    "\trev = rev.lower() #text to lowercase\n",
    "\trev = rev.split() #each word of the sentence becomes the element of a list\n",
    "\trev = [wlemmatizer.lemmatize(word) for word in rev if word not in stopwords.words('english')] #lemmatization via list comprehension\n",
    "\trev = ' '.join(rev) #from list to string\n",
    "\tcorpus.append(rev) #appending to the list\n",
    "    \n",
    "#ngram_range=(3, 3)\n",
    "cv = CountVectorizer(ngram_range=(1, 2), max_features=int(length/10)) \n",
    "x = cv.fit_transform(corpus).toarray() #converting to array\n",
    "y = data['Habituality'] #dependent variable\n",
    "\n",
    "\n",
    "## y is a categorical variable so will encode it\n",
    "#le = LabelEncoder()\n",
    "#y = le.fit_transform(y)\n",
    "\n",
    "## now splitting the model into train and test set\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47ae695",
   "metadata": {},
   "outputs": [],
   "source": [
    "## training the model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB() # using naive bayes classification algorithm \n",
    "predict_fit = model.fit(x,y)# fitting the model\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10)\n",
    "predictions = cross_val_predict(model, x, y, cv=kfold)\n",
    "target_names = ['nonhabitual', 'habitual']\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y, predictions)\n",
    "\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41c9043",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59817b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y, predictions, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee749b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f10bbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b69155",
   "metadata": {},
   "outputs": [],
   "source": [
    "beNum = []\n",
    "for Sentence in sen:\n",
    "    numBe = 0\n",
    "    if nlp(Sentence.text) != parsed:\n",
    "        parsed = nlp(Sentence.text)\n",
    "        for i, word in enumerate(parsed):\n",
    "            if (word.text.lower() == \"be\"):\n",
    "                numBe += 1\n",
    "    beNum.append([Sentence.text,numBe])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7982809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in beNum:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff17994",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(beNum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19113859",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parsed)\n",
    "print(nlp(Sentence.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1346b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in extraBe:\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724a537d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067a8cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.data import load\n",
    "tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "#tagdict['NN'][0]\n",
    "tagdict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba1595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping numerical values to various part of speech list\n",
    "pos_list = pd.DataFrame({'pos': dir(spacy.parts_of_speech)})\n",
    "#pos_list = copy.deepcopy(dir(spacy.parts_of_speech))\n",
    "num = []\n",
    "for i in range((len(pos_list))):\n",
    "    num.append(i)\n",
    "pos_list.insert(1, 'num', num)\n",
    "print(pos_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cecfdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping the same numerical values to the part of speech of te actual words in the sentences before and after 'be'\n",
    "for i in range(len(w)):\n",
    "    for j in range(len(pos_list)):\n",
    "        if (w[i] == pos_list.iloc[j, 0]):\n",
    "            w[i] = pos_list.iloc[j, 1]\n",
    "        if (pppp[i] == pos_list.iloc[j, 0]):\n",
    "            pppp[i] = pos_list.iloc[j, 1]\n",
    "        if (ppp[i] == pos_list.iloc[j, 0]):\n",
    "            ppp[i] = pos_list.iloc[j, 1]\n",
    "        if (pp[i] == pos_list.iloc[j, 0]):\n",
    "            pp[i] = pos_list.iloc[j, 1]\n",
    "        if (p[i] == pos_list.iloc[j, 0]):\n",
    "            p[i] = pos_list.iloc[j, 1]\n",
    "        if (a[i] == pos_list.iloc[j, 0]):\n",
    "            a[i] = pos_list.iloc[j, 1]\n",
    "        if (pa[i] == pos_list.iloc[j, 0]):\n",
    "            pa[i] = pos_list.iloc[j, 1]\n",
    "        if (ppa[i] == pos_list.iloc[j, 0]):\n",
    "            ppa[i] = pos_list.iloc[j, 1]\n",
    "        if (pppa[i] == pos_list.iloc[j, 0]):\n",
    "            pppa[i] = pos_list.iloc[j, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062f82b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data.drop(columns=['n-gram'], axis = 1)\n",
    "#data = data.drop(columns=['word', 'preprepreprevious', 'prepreprevious', 'preprevious', 'previous','after','postafter', 'postpostafter', 'postpostpostafter'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98ea1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the data that has the  n-gram,and all the part of speech mappings\n",
    "data.insert(len(data.columns), \"n-gram\", predictions)\n",
    "data.insert(len(data.columns), \"word\", w)\n",
    "data.insert(len(data.columns), \"preprepreprevious\", pppp)\n",
    "data.insert(len(data.columns), \"prepreprevious\", ppp)\n",
    "data.insert(len(data.columns), \"preprevious\", pp)\n",
    "data.insert(len(data.columns), \"previous\", p)\n",
    "data.insert(len(data.columns), \"after\", a)\n",
    "data.insert(len(data.columns), \"postafter\", pa)\n",
    "data.insert(len(data.columns), \"postpostafter\", ppa)\n",
    "data.insert(len(data.columns), \"postpostpostafter\", pppa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ec4b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['Habituality', 'Sentence'], axis=1)\n",
    "y = data['Habituality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520335ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e0447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the pipeline model\n",
    "scaler = StandardScaler()\n",
    "lr = LogisticRegression(class_weight='balanced', max_iter = 500)\n",
    "estimators=[('lr', LogisticRegression(max_iter = 10000, class_weight='balanced')), ('mlp', MLPClassifier(max_iter = 10000, solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)), ('svc', SVC(probability=True, max_iter = 10000, class_weight='balanced'))]\n",
    "#create our voting classifier, inputting our models\n",
    "ensemble = VotingClassifier(estimators, voting='soft')\n",
    "model4 = Pipeline([('standardize', scaler),\n",
    "                    ('ensemble', ensemble)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75288a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'C':('ensemble', 'rbf'), 'C': [1, 10]}\n",
    "#model4 = GridSearchCV(lr, parameters, scoring='recall_weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf2f0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10)\n",
    "predictions = cross_val_predict(model4, X, y, cv=kfold)\n",
    "target_names = ['nonhabitual', 'habitual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e64449",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fold_actual = [y[j] for i, j in kfold.split(X,y)]\n",
    "fold_pred = [predictions[j] for i, j in kfold.split(X,y)]\n",
    "each_f = []\n",
    "f_score_summary = []\n",
    "for i in range(len(fold_actual)):\n",
    "    fold_actual[i] = np.array(fold_actual[i])\n",
    "    report = classification_report(fold_actual[i], fold_pred[i], target_names=target_names, output_dict=True)\n",
    "    each_f.extend((report['nonhabitual']['f1-score'], report['habitual']['f1-score'], report['weighted avg']['f1-score']))\n",
    "    f_score_summary.append(each_f)\n",
    "    each_f = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e3b15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = data['Sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b053e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold_train_i = pd.DataFrame()\n",
    "k_fold_train_eo = pd.DataFrame()\n",
    "k_fold_test_i = pd.DataFrame()\n",
    "k_fold_test_eo = pd.DataFrame()\n",
    "count = 0\n",
    "for i, j in kfold.split(X,y):\n",
    "    y_train, y_test = y[i], y[j]\n",
    "    z_train, z_test = z[i], z[j]\n",
    "    x_train, x_test = X.iloc[i], X.iloc[j]\n",
    "    train_input_csv = 'Augmented Dependency Kfold Baseline 2' + str(count) + ' Train Input.csv'\n",
    "    train_output_csv = 'Augmented Dependency Kfold Baseline 2' + str(count) + ' Train Expected Output.csv'\n",
    "    test_input_csv = 'Augmented Dependency Kfold Baseline 2' + str(count) + ' Test Input.csv'\n",
    "    test_output_csv = 'Augmented Dependency Kfold Baseline 2' + str(count) + ' Test Expected Output.csv'\n",
    "    with open(train_input_csv, 'w+', newline='') as file:\n",
    "       x_train.to_csv(train_input_csv)\n",
    "    with open(train_output_csv, 'w+', newline='') as file:\n",
    "       y_train.to_csv(train_output_csv)\n",
    "    with open(test_input_csv, 'w+', newline='') as file:\n",
    "       x_test.to_csv(test_input_csv)\n",
    "    with open(test_output_csv, 'w+', newline='') as file:\n",
    "       y_test.to_csv(test_output_csv)\n",
    "    count += 1\n",
    "\n",
    "\n",
    "#k_fold_train = pd.concat([k_fold_input, z[j]])\n",
    "#OK ABOVE JUST GIVES TRAIN/TEST FOR ONE GOTTA DO IT FO RALL\n",
    "    \n",
    "#k_fold_input = pd.concat([k_fold_input, z[j]])\n",
    "#k_fold_expected_output = pd.concat([k_fold_expected_output, y[j]])\n",
    "    \n",
    "    \n",
    "#for train_index, test_index in skf.split(X, y):\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ff32d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame(f_score_summary)\n",
    "#df.plot(kind='box')\n",
    "non_hab = []\n",
    "hab = []\n",
    "avg = []\n",
    "for i in range(len(f_score_summary)):\n",
    "    non_hab.append(f_score_summary[i][0])\n",
    "    hab.append(f_score_summary[i][1])\n",
    "    avg.append(f_score_summary[i][2])\n",
    "nh_max = max(non_hab)\n",
    "h_max = max(hab)\n",
    "wa_max = max(avg)\n",
    "nh_min = min(non_hab)\n",
    "h_min = min(hab)\n",
    "wa_min = min(avg)\n",
    "print(nh_max, h_max, wa_max, nh_min, h_min, wa_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557ba4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#f_score_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bfaee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b9fde2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(y, predictions, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c965e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10)\n",
    "prediction_probs = cross_val_predict(model4, X, y, cv=kfold, method = 'predict_proba')\n",
    "prediction_new = copy.deepcopy(prediction_probs)\n",
    "prediction_new = prediction_new[:,0]\n",
    "for i in range(len(prediction_probs)):\n",
    "    # 0.79\n",
    "    if (prediction_new[i] >= 0.84):#0.64 #was 0.94\n",
    "        prediction_new[i] = -1\n",
    "    else:\n",
    "        prediction_new[i] = 1\n",
    "#change value if number rules changed!!!!\n",
    "#for i in range(len(prediction_probs)):\n",
    "#    if (data_new.iloc[i, 13] == 1 and data_new.iloc[i, 15] == 1):\n",
    "#        prediction_new[i] = 1\n",
    "        \n",
    "data_new = copy.deepcopy(data)\n",
    "data_new.insert(1, 'Probability Non-Habitual', prediction_probs[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf9060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit = model4.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9334d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#joblib.dump(model_fit, \"harrison habituality_model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adec1ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('new predicted coraal_analysis_spreadsheet.csv', 'w+', newline='') as file:\n",
    "#   data_new.to_csv('new predicted coraal_analysis_spreadsheet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50c9576",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(data_new)):\n",
    "#   data_new.iat[i,0] = prediction_new[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc31474",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y, prediction_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08061b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y, prediction_new, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4814dbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#falsePredictedNewData = copy.deepcopy(data_new)\n",
    "#newPredictedNewData = copy.deepcopy(data_new)\n",
    "#posPredictedNewData = copy.deepcopy(data_new)\n",
    "#i = len(falsePredictedNewData)-1\n",
    "#while (i >= 0):\n",
    "#    falsePredictedNewData.drop(i, inplace=True)\n",
    "#    posPredictedNewData.drop(i, inplace=True)\n",
    "#    i = i -1\n",
    "#for i in range(len(data_new)):\n",
    "#    if (data_new.loc[i, 'Habituality'] != data.loc[i, 'Habituality']):\n",
    "#        falsePredictedNewData = falsePredictedNewData.append(data_new.iloc[i], ignore_index=True)\n",
    "#    else:\n",
    "#        posPredictedNewData = posPredictedNewData.append(data_new.iloc[i], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb03268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('false predicted new coraal_analysis_spreadsheet.csv', 'w+', newline='') as file:\n",
    "#   falsePredictedNewData.to_csv('false predicted new coraal_analysis_spreadsheet.csv')#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
